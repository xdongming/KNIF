{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "050e659e-20f4-4b07-b724-21cb2ef26b76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from torch.func import vmap, jacrev\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "import math\n",
    "from pydmd import DMD\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2a57628-6d4f-47de-842f-e5d20fae90a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_dim=128, n_layers=2, activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(in_dim, hidden_dim), activation()]\n",
    "        for _ in range(n_layers - 1):\n",
    "            layers.extend([nn.Linear(hidden_dim, hidden_dim), activation()])\n",
    "        layers.append(nn.Linear(hidden_dim, out_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, hidden_dim=128, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.net = MLP(input_dim, latent_dim, hidden_dim, n_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim, hidden_dim=128, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.net = MLP(latent_dim, output_dim, hidden_dim, n_layers)\n",
    "\n",
    "    def forward(self, y):\n",
    "        return self.net(y)\n",
    "\n",
    "class AuxiliaryKoopmanNet(nn.Module):\n",
    "    def __init__(self, koopman_type='complex', hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.koopman_type = koopman_type\n",
    "        in_dim = 2 if koopman_type == 'complex' else 1  \n",
    "        out_dim = 2 if koopman_type == 'complex' else 1\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, y):\n",
    "        out = self.net(y)\n",
    "        if self.koopman_type == 'complex':\n",
    "            mu = out[:, 0]\n",
    "            omega = out[:, 1]\n",
    "            return mu, omega\n",
    "        else:\n",
    "            mu = out.squeeze(-1)\n",
    "            return mu, None\n",
    "\n",
    "def build_koopman_matrix(mu, omega):\n",
    "    batch_size = mu.shape[0]\n",
    "    dt = 1.0\n",
    "    exp_mu = torch.exp(mu * dt)\n",
    "    if omega is not None:\n",
    "        cos_ = torch.cos(omega * dt)\n",
    "        sin_ = torch.sin(omega * dt)\n",
    "        K_blocks = torch.zeros(batch_size, 2, 2).to(mu.device)\n",
    "        K_blocks[:, 0, 0] = exp_mu * cos_\n",
    "        K_blocks[:, 0, 1] = -exp_mu * sin_\n",
    "        K_blocks[:, 1, 0] = exp_mu * sin_\n",
    "        K_blocks[:, 1, 1] = exp_mu * cos_\n",
    "    else:\n",
    "        K_blocks = torch.eye(2).unsqueeze(0).repeat(batch_size, 1, 1).to(mu.device)\n",
    "        K_blocks = exp_mu.view(-1, 1, 1) * K_blocks\n",
    "    return K_blocks\n",
    "\n",
    "class DeepKoopman(nn.Module):\n",
    "    def __init__(self, input_dim, num_real, num_complex, hidden_dim=128, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.num_real = num_real\n",
    "        self.num_complex = num_complex\n",
    "        self.latent_dim = num_real * 1 + num_complex * 2\n",
    "\n",
    "        self.encoder = Encoder(input_dim, self.latent_dim, hidden_dim, n_layers)\n",
    "        self.decoder = Decoder(self.latent_dim, input_dim, hidden_dim, n_layers)\n",
    "        self.aux_nets = nn.ModuleList()\n",
    "\n",
    "        for _ in range(num_real):\n",
    "            self.aux_nets.append(AuxiliaryKoopmanNet(koopman_type='real'))\n",
    "        for _ in range(num_complex):\n",
    "            self.aux_nets.append(AuxiliaryKoopmanNet(koopman_type='complex'))\n",
    "\n",
    "    def split_latent(self, z):\n",
    "        idx = 0\n",
    "        parts = []\n",
    "        for _ in range(self.num_real):\n",
    "            parts.append(z[:, idx:idx+1])\n",
    "            idx += 1\n",
    "        for _ in range(self.num_complex):\n",
    "            parts.append(z[:, idx:idx+2])\n",
    "            idx += 2\n",
    "        return parts\n",
    "\n",
    "    def koopman_step(self, z):\n",
    "        zs = self.split_latent(z)\n",
    "        updated = []\n",
    "        for i, (sub_z, net) in enumerate(zip(zs, self.aux_nets)):\n",
    "            mu, omega = net(sub_z)\n",
    "            if sub_z.shape[1] == 1:\n",
    "                z_next = mu.unsqueeze(-1) * sub_z\n",
    "            else:\n",
    "                B = build_koopman_matrix(mu, omega)\n",
    "                z_rot = torch.bmm(B, sub_z.unsqueeze(-1)).squeeze(-1)\n",
    "                z_next = z_rot\n",
    "            updated.append(z_next)\n",
    "        return torch.cat(updated, dim=-1)\n",
    "\n",
    "    def koopman_power(self, z, m):\n",
    "        for _ in range(m):\n",
    "            z = self.koopman_step(z)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x, steps=1, reverse=False):\n",
    "        if reverse:\n",
    "            raise NotImplementedError(\"Reverse is not used here.\")\n",
    "        z0 = self.encoder(x)\n",
    "        z1 = self.koopman_power(z0, steps)\n",
    "        x_pred = self.decoder(z1)\n",
    "        return x_pred, z0, z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8996c880-1612-417a-ade9-b294f93ad045",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def koopman_rollout_prediction(model, X):\n",
    "    \"\"\"\n",
    "    Koopman rollout prediction over full sequence length T.\n",
    "\n",
    "    Args:\n",
    "        X: shape [B, T, dim]\n",
    "    Returns:\n",
    "        x_preds: shape [B, T, dim]\n",
    "        y_preds: shape [B, T, latent_dim]\n",
    "    \"\"\"\n",
    "    B, T, dim = X.shape\n",
    "    x0 = X[:, 0, :]\n",
    "    y = model.encoder(x0)  # [B, latent_dim]\n",
    "\n",
    "    x_preds = []\n",
    "    y_preds = []\n",
    "\n",
    "    for t in range(T):\n",
    "        x_hat = model.decoder(y)\n",
    "        x_preds.append(x_hat)\n",
    "        y_preds.append(y)\n",
    "\n",
    "        mu, omega = model.aux_net(y)\n",
    "        K = build_koopman_matrix(mu, omega)  # [B, 2, 2]\n",
    "\n",
    "        y_next = y.clone()\n",
    "        y_rot = torch.bmm(K, y[:, :2].unsqueeze(-1)).squeeze(-1)\n",
    "        y_next[:, :2] = y_rot\n",
    "        y = y_next\n",
    "\n",
    "    x_preds = torch.stack(x_preds, dim=1)  # [B, T, dim]\n",
    "    y_preds = torch.stack(y_preds, dim=1)  # [B, T, latent_dim]\n",
    "    return x_preds, y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bede68be-e48a-4205-b695-7cc047d892cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TrainModel(pl.LightningModule):\n",
    "    def __init__(self, model, dim, learning_rate=1e-3,\n",
    "                 alpha1=0.1, alpha2=1e-7, alpha3=1e-13, \n",
    "                 path=\"deepkoop_ckpt\"):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.alpha1 = alpha1\n",
    "        self.alpha2 = alpha2\n",
    "        self.alpha3 = alpha3\n",
    "        self.criterion_mse = nn.MSELoss()\n",
    "        self.criterion_linf = lambda a, b: torch.max(torch.abs(a - b))\n",
    "        self.path = path + \".ckpt\"\n",
    "        self.dim = dim\n",
    "\n",
    "        self.best_val_loss = float(\"inf\")\n",
    "        self.validation_outputs = []\n",
    "        self.train_losses = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def compute_loss(self, x):\n",
    "        B, T, dim = x.shape\n",
    "        x1 = x[:, 0]\n",
    "        x2 = x[:, 1]\n",
    "        z1 = self.model.encoder(x1)\n",
    "        x1_recon = self.model.decoder(z1)\n",
    "\n",
    "        # Reconstruction Loss\n",
    "        L_recon = self.criterion_mse(x1[:, :self.dim], x1_recon[:, :self.dim])\n",
    "\n",
    "        # Future Prediction Loss\n",
    "        L_pred = 0\n",
    "        for m in range(1, T):\n",
    "            z_pred = self.model.koopman_power(z1, m)  # K^m Ï†(x1)\n",
    "            x_pred = self.model.decoder(z_pred)\n",
    "            L_pred += self.criterion_mse(x[:, m, :self.dim], x_pred[:, :self.dim])\n",
    "        L_pred /= (T - 1)\n",
    "\n",
    "        # Linearity Loss\n",
    "        L_lin = 0\n",
    "        z_all = self.model.encoder(x)\n",
    "        for m in range(T - 1):\n",
    "            z_next_pred = self.model.koopman_step(z_all[:, m])\n",
    "            L_lin += self.criterion_mse(z_all[:, m + 1], z_next_pred)\n",
    "        L_lin /= (T - 1)\n",
    "\n",
    "        # L_inf Loss\n",
    "        x2_pred = self.model.decoder(self.model.koopman_power(z1, 1))\n",
    "        L_inf = self.criterion_linf(x1[:, :self.dim], x1_recon[:, :self.dim]) + self.criterion_linf(x2[:, :self.dim], x2_pred[:, :self.dim])\n",
    "\n",
    "        # L2 regularization\n",
    "        l2_reg = sum(torch.norm(p, 2) ** 2 for p in self.model.parameters() if p.requires_grad)\n",
    "\n",
    "        loss = self.alpha1 * (L_recon + L_pred) + L_lin + self.alpha2 * L_inf + self.alpha3 * l2_reg\n",
    "        return loss, L_recon, L_pred, L_lin, L_inf\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X = batch[0]  # shape: [B, T, dim]\n",
    "        loss, L_recon, L_pred, L_lin, L_inf = self.compute_loss(X)\n",
    "        self.log_dict({\n",
    "            'train_loss': loss,\n",
    "            'train_L_recon': L_recon,\n",
    "            'train_L_pred': L_pred,\n",
    "            'train_L_lin': L_lin,\n",
    "            'train_L_inf': L_inf\n",
    "        }, on_step=True, on_epoch=False, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X = batch[0]  \n",
    "        _, _, L_pred, _, _ = self.compute_loss(X)\n",
    "        self.validation_outputs.append(L_pred)\n",
    "        self.log('val_loss', L_pred)\n",
    "        return L_pred\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        X = batch[0]  \n",
    "        _, _, L_pred, _, _ = self.compute_loss(X)\n",
    "        self.log(\"test_loss\", L_pred)\n",
    "        return L_pred\n",
    "\n",
    "    def on_fit_start(self):\n",
    "        if self.trainer.is_global_zero:\n",
    "            if os.path.exists(\"loss_log.txt\"):\n",
    "                os.remove(\"loss_log.txt\")\n",
    "            if os.path.exists(self.path):\n",
    "                os.remove(self.path)\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        if self.trainer.is_global_zero:\n",
    "            avg_train_loss = self.trainer.callback_metrics.get(\"train_loss\")\n",
    "            if avg_train_loss is not None:\n",
    "                self.train_losses.append(avg_train_loss.item())\n",
    "                print(f\"Epoch {self.current_epoch}: Average Training Loss = {avg_train_loss.item()}\")\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_val_loss = torch.stack(self.validation_outputs).mean()\n",
    "        self.log('avg_val_loss', avg_val_loss)\n",
    "        self.validation_outputs.clear()\n",
    "        print(f\"Validation loss: {avg_val_loss}\")\n",
    "        with open(\"loss_log.txt\", \"a\") as f:\n",
    "            f.write(f\"{avg_val_loss.item()}\\n\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate, eps=1e-8, weight_decay=0)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.92)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\"\n",
    "            },\n",
    "            \"gradient_clip_val\": 1.0,\n",
    "            \"gradient_clip_algorithm\": \"norm\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b602e280-468b-4cf2-a863-55ec84de6de8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dim = 6  \n",
    "hidden_dim = 80  \n",
    "input_dim = 6\n",
    "n_blocks = 3  \n",
    "n_layers = 3\n",
    "num_real = 18\n",
    "num_complex = 1\n",
    "rank = 50\n",
    "batch_size = 256\n",
    "n_train = 6660\n",
    "n_test = 606\n",
    "dropout = 0\n",
    "num_epochs = 100\n",
    "lamb = 0\n",
    "learning_rate = 1e-3  # å­¦ä¹ çŽ‡\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6874831a-16e4-4744-8cc9-1cf2c819a392",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('robot_X_train.csv', header=None).values\n",
    "X_test = pd.read_csv('robot_X_test.csv', header=None).values\n",
    "U_train = pd.read_csv('robot_U_train.csv', header=None).values\n",
    "U_test = pd.read_csv('robot_U_test.csv', header=None).values\n",
    "\n",
    "length = X_train.shape[1] // n_train\n",
    "HX_train = []\n",
    "HU_train = []\n",
    "for i in range(n_train):\n",
    "    HX_train.append(X_train[:, i*length:(i+1)*length])\n",
    "    HU_train.append(U_train[:, i*length:(i+1)*length])\n",
    "HX_train = np.stack([HX_train[idx].T for idx in range(n_train)], axis=0)\n",
    "HU_train = np.stack([HU_train[idx].T for idx in range(n_train)], axis=0)\n",
    "H_train = np.concatenate([HX_train, HU_train], axis=-1)\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(H_train, dtype=torch.float32))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18639093-8bd2-44b5-82b7-ad2fabbe46d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "path = \"model_checkpoint_Robot_deepkoopman\"\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"avg_val_loss\",   \n",
    "    dirpath=\"./\",\n",
    "    filename=path,  \n",
    "    save_top_k=1,  \n",
    "    mode=\"min\",    \n",
    ")\n",
    "model = DeepKoopman(\n",
    "    input_dim=dim+input_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    n_layers=n_layers,\n",
    "    num_real=num_real,\n",
    "    num_complex=num_complex\n",
    ")\n",
    "lightning_model = TrainModel(model=model, dim=dim, learning_rate=learning_rate, path=path)\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", devices=4, strategy=\"ddp_notebook\", max_epochs=num_epochs, callbacks=[checkpoint_callback])\n",
    "trainer.fit(lightning_model, train_loader, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8abde0b2-c523-4bad-830f-ae0a236d0f44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "model = DeepKoopman(input_dim=dim+input_dim, hidden_dim=hidden_dim, n_layers=n_layers, num_real=num_real, num_complex=num_complex)\n",
    "path = \"model_checkpoint_Robot_deepkoopman.ckpt\"\n",
    "lightning_model = TrainModel(model=model, dim=dim, learning_rate=learning_rate, path=path)\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", devices=4, strategy=\"ddp_notebook\", max_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54364185-0c2f-433c-9277-17d4419aa82b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "length = X_test.shape[1] // n_test\n",
    "HX_test = []\n",
    "HU_test = []\n",
    "for i in range(n_test):\n",
    "    HX_test.append(X_test[:, i*length:(i+1)*length])\n",
    "    HU_test.append(U_test[:, i*length:(i+1)*length])\n",
    "HX_test = np.stack([HX_test[idx].T for idx in range(n_test)], axis=0)\n",
    "HU_test = np.stack([HU_test[idx].T for idx in range(n_test)], axis=0)\n",
    "H_test = np.concatenate([HX_train, HU_train], axis=-1)\n",
    "test_dataset = TensorDataset(torch.tensor(H_test, dtype=torch.float32))\n",
    "test_loader = DataLoader(test_dataset, batch_size=9999, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b65bcc7-4cda-4e60-8fe3-8b08b994f587",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n",
      "You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 4 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.35it/s]\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "       Test metric             DataLoader 0\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "        test_loss            446.3254089355469\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 446.3254089355469}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "trainer.test(lightning_model, dataloaders=test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
