{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "050e659e-20f4-4b07-b724-21cb2ef26b76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from torch.func import vmap, jacrev\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "import math\n",
    "from pydmd import DMD\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b602e280-468b-4cf2-a863-55ec84de6de8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_dim=128, n_layers=2, activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(in_dim, hidden_dim), activation()]\n",
    "        for _ in range(n_layers - 1):\n",
    "            layers.extend([nn.Linear(hidden_dim, hidden_dim), activation()])\n",
    "        layers.append(nn.Linear(hidden_dim, out_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, hidden_dim=128, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.net = MLP(input_dim, latent_dim, hidden_dim, n_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim, hidden_dim=128, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.net = MLP(latent_dim, output_dim, hidden_dim, n_layers)\n",
    "\n",
    "    def forward(self, y):\n",
    "        return self.net(y)\n",
    "\n",
    "class AuxiliaryKoopmanNet(nn.Module):\n",
    "    def __init__(self, koopman_type='complex', hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.koopman_type = koopman_type\n",
    "        in_dim = 2 if koopman_type == 'complex' else 1  \n",
    "        out_dim = 2 if koopman_type == 'complex' else 1\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, y):\n",
    "        out = self.net(y)\n",
    "        if self.koopman_type == 'complex':\n",
    "            mu = out[:, 0]\n",
    "            omega = out[:, 1]\n",
    "            return mu, omega\n",
    "        else:\n",
    "            mu = out.squeeze(-1)\n",
    "            return mu, None\n",
    "\n",
    "def build_koopman_matrix(mu, omega):\n",
    "    batch_size = mu.shape[0]\n",
    "    dt = 1.0\n",
    "    exp_mu = torch.exp(mu * dt)\n",
    "    if omega is not None:\n",
    "        cos_ = torch.cos(omega * dt)\n",
    "        sin_ = torch.sin(omega * dt)\n",
    "        K_blocks = torch.zeros(batch_size, 2, 2).to(mu.device)\n",
    "        K_blocks[:, 0, 0] = exp_mu * cos_\n",
    "        K_blocks[:, 0, 1] = -exp_mu * sin_\n",
    "        K_blocks[:, 1, 0] = exp_mu * sin_\n",
    "        K_blocks[:, 1, 1] = exp_mu * cos_\n",
    "    else:\n",
    "        K_blocks = torch.eye(2).unsqueeze(0).repeat(batch_size, 1, 1).to(mu.device)\n",
    "        K_blocks = exp_mu.view(-1, 1, 1) * K_blocks\n",
    "    return K_blocks\n",
    "\n",
    "class DeepKoopman(nn.Module):\n",
    "    def __init__(self, input_dim, num_real, num_complex, hidden_dim=128, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.num_real = num_real\n",
    "        self.num_complex = num_complex\n",
    "        self.latent_dim = num_real * 1 + num_complex * 2\n",
    "\n",
    "        self.encoder = Encoder(input_dim, self.latent_dim, hidden_dim, n_layers)\n",
    "        self.decoder = Decoder(self.latent_dim, input_dim, hidden_dim, n_layers)\n",
    "        self.aux_nets = nn.ModuleList()\n",
    "\n",
    "        for _ in range(num_real):\n",
    "            self.aux_nets.append(AuxiliaryKoopmanNet(koopman_type='real'))\n",
    "        for _ in range(num_complex):\n",
    "            self.aux_nets.append(AuxiliaryKoopmanNet(koopman_type='complex'))\n",
    "\n",
    "    def split_latent(self, z):\n",
    "        idx = 0\n",
    "        parts = []\n",
    "        for _ in range(self.num_real):\n",
    "            parts.append(z[:, idx:idx+1])\n",
    "            idx += 1\n",
    "        for _ in range(self.num_complex):\n",
    "            parts.append(z[:, idx:idx+2])\n",
    "            idx += 2\n",
    "        return parts\n",
    "\n",
    "    def koopman_step(self, z):\n",
    "        zs = self.split_latent(z)\n",
    "        updated = []\n",
    "        for i, (sub_z, net) in enumerate(zip(zs, self.aux_nets)):\n",
    "            mu, omega = net(sub_z)\n",
    "            if sub_z.shape[1] == 1:\n",
    "                z_next = mu.unsqueeze(-1) * sub_z\n",
    "            else:\n",
    "                B = build_koopman_matrix(mu, omega)\n",
    "                z_rot = torch.bmm(B, sub_z.unsqueeze(-1)).squeeze(-1)\n",
    "                z_next = z_rot\n",
    "            updated.append(z_next)\n",
    "        return torch.cat(updated, dim=-1)\n",
    "\n",
    "    def koopman_power(self, z, m):\n",
    "        for _ in range(m):\n",
    "            z = self.koopman_step(z)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x, steps=1, reverse=False):\n",
    "        if reverse:\n",
    "            raise NotImplementedError(\"Reverse is not used here.\")\n",
    "        z0 = self.encoder(x)\n",
    "        z1 = self.koopman_power(z0, steps)\n",
    "        x_pred = self.decoder(z1)\n",
    "        return x_pred, z0, z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92f82eb2-731c-4827-9194-6a65f9842548",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def koopman_rollout_prediction(model, X):\n",
    "    \"\"\"\n",
    "    Koopman rollout prediction over full sequence length T.\n",
    "\n",
    "    Args:\n",
    "        X: shape [B, T, dim]\n",
    "    Returns:\n",
    "        x_preds: shape [B, T, dim]\n",
    "        y_preds: shape [B, T, latent_dim]\n",
    "    \"\"\"\n",
    "    B, T, dim = X.shape\n",
    "    x0 = X[:, 0, :]  \n",
    "    y = model.encoder(x0)  # [B, latent_dim]\n",
    "\n",
    "    x_preds = []\n",
    "    y_preds = []\n",
    "\n",
    "    for t in range(T):\n",
    "        x_hat = model.decoder(y)\n",
    "        x_preds.append(x_hat)\n",
    "        y_preds.append(y)\n",
    "\n",
    "        mu, omega = model.aux_net(y)\n",
    "        K = build_koopman_matrix(mu, omega)  # [B, 2, 2]\n",
    "\n",
    "        y_next = y.clone()\n",
    "        y_rot = torch.bmm(K, y[:, :2].unsqueeze(-1)).squeeze(-1)\n",
    "        y_next[:, :2] = y_rot\n",
    "        y = y_next\n",
    "\n",
    "    x_preds = torch.stack(x_preds, dim=1)  # [B, T, dim]\n",
    "    y_preds = torch.stack(y_preds, dim=1)  # [B, T, latent_dim]\n",
    "    return x_preds, y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c43d5609-1fb3-4f6c-a55b-23216203377a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TrainModel(pl.LightningModule):\n",
    "    def __init__(self, model, learning_rate=1e-3,\n",
    "                 alpha1=0.1, alpha2=1e-7, alpha3=1e-13, \n",
    "                 path=\"deepkoop_ckpt\"):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.alpha1 = alpha1\n",
    "        self.alpha2 = alpha2\n",
    "        self.alpha3 = alpha3\n",
    "        self.criterion_mse = nn.MSELoss()\n",
    "        self.criterion_linf = lambda a, b: torch.max(torch.abs(a - b))\n",
    "        self.path = path + \".ckpt\"\n",
    "\n",
    "        self.best_val_loss = float(\"inf\")\n",
    "        self.validation_outputs = []\n",
    "        self.train_losses = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def compute_loss(self, x):\n",
    "        B, T, dim = x.shape\n",
    "        x1 = x[:, 0]\n",
    "        x2 = x[:, 1]\n",
    "        z1 = self.model.encoder(x1)\n",
    "        x1_recon = self.model.decoder(z1)\n",
    "\n",
    "        # Reconstruction Loss\n",
    "        L_recon = self.criterion_mse(x1, x1_recon)\n",
    "\n",
    "        # Future Prediction Loss\n",
    "        L_pred = 0\n",
    "        for m in range(1, T):\n",
    "            z_pred = self.model.koopman_power(z1, m)  # K^m Ï†(x1)\n",
    "            x_pred = self.model.decoder(z_pred)\n",
    "            L_pred += self.criterion_mse(x[:, m], x_pred)\n",
    "        L_pred /= (T - 1)\n",
    "\n",
    "        # Linearity Loss\n",
    "        L_lin = 0\n",
    "        z_all = self.model.encoder(x)\n",
    "        for m in range(T - 1):\n",
    "            z_next_pred = self.model.koopman_step(z_all[:, m])\n",
    "            L_lin += self.criterion_mse(z_all[:, m + 1], z_next_pred)\n",
    "        L_lin /= (T - 1)\n",
    "\n",
    "        # L_inf Loss\n",
    "        x2_pred = self.model.decoder(self.model.koopman_power(z1, 1))\n",
    "        L_inf = self.criterion_linf(x1, x1_recon) + self.criterion_linf(x2, x2_pred)\n",
    "\n",
    "        # L2 regularization\n",
    "        l2_reg = sum(torch.norm(p, 2) ** 2 for p in self.model.parameters() if p.requires_grad)\n",
    "\n",
    "        loss = self.alpha1 * (L_recon + L_pred) + L_lin + self.alpha2 * L_inf + self.alpha3 * l2_reg\n",
    "        return loss, L_recon, L_pred, L_lin, L_inf\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X = batch[0]  # shape: [B, T, dim]\n",
    "        loss, L_recon, L_pred, L_lin, L_inf = self.compute_loss(X)\n",
    "        self.log_dict({\n",
    "            'train_loss': loss,\n",
    "            'train_L_recon': L_recon,\n",
    "            'train_L_pred': L_pred,\n",
    "            'train_L_lin': L_lin,\n",
    "            'train_L_inf': L_inf\n",
    "        }, on_step=True, on_epoch=False, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X = batch[0] \n",
    "        _, _, L_pred, _, _ = self.compute_loss(X)\n",
    "        self.validation_outputs.append(L_pred)\n",
    "        self.log('val_loss', L_pred)\n",
    "        return L_pred\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        X = batch[0]  # shape: [B, T, dim]\n",
    "        _, _, L_pred, _, _ = self.compute_loss(X)\n",
    "        self.log(\"test_loss\", L_pred)\n",
    "        return L_pred\n",
    "\n",
    "    def on_fit_start(self):\n",
    "        if self.trainer.is_global_zero:\n",
    "            if os.path.exists(\"loss_log.txt\"):\n",
    "                os.remove(\"loss_log.txt\")\n",
    "            if os.path.exists(self.path):\n",
    "                os.remove(self.path)\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        if self.trainer.is_global_zero:\n",
    "            avg_train_loss = self.trainer.callback_metrics.get(\"train_loss\")\n",
    "            if avg_train_loss is not None:\n",
    "                self.train_losses.append(avg_train_loss.item())\n",
    "                print(f\"Epoch {self.current_epoch}: Average Training Loss = {avg_train_loss.item()}\")\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_val_loss = torch.stack(self.validation_outputs).mean()\n",
    "        self.log('avg_val_loss', avg_val_loss)\n",
    "        self.validation_outputs.clear()\n",
    "        print(f\"Validation loss: {avg_val_loss}\")\n",
    "        with open(\"loss_log.txt\", \"a\") as f:\n",
    "            f.write(f\"{avg_val_loss.item()}\\n\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate, eps=1e-8, weight_decay=0)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.92)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\"\n",
    "            },\n",
    "            \"gradient_clip_val\": 1.0,\n",
    "            \"gradient_clip_algorithm\": \"norm\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd91c08d-d518-4473-85da-d92ab90d7dd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dim = 3  \n",
    "hidden_dim = 40  \n",
    "input_dim = 0\n",
    "n_blocks = 3  \n",
    "n_layers = 3\n",
    "num_real = 3\n",
    "num_complex = 1\n",
    "batch_size = 512\n",
    "n_train = 10000\n",
    "n_valid = 1000\n",
    "n_test = 1000\n",
    "dropout = 0\n",
    "num_epochs = 20  \n",
    "lamb = 0\n",
    "learning_rate = 1e-3  \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38b85bfb-a714-4750-9b83-03efae756757",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('KO_sample_train.csv', header=None).values\n",
    "X_valid = pd.read_csv('KO_sample_valid.csv', header=None).values\n",
    "X_test = pd.read_csv('KO_sample_test.csv', header=None).values\n",
    "\n",
    "length = X_train.shape[1] // n_train\n",
    "H_train = []\n",
    "for i in range(n_train):\n",
    "    H_train.append(X_train[:, i*length:(i+1)*length])\n",
    "H_train = np.stack([H_train[idx].T for idx in range(n_train)], axis=0)\n",
    "H_valid = []\n",
    "for i in range(n_valid):\n",
    "    H_valid.append(X_valid[:, i*length:(i+1)*length])\n",
    "H_valid = np.stack([H_valid[idx].T for idx in range(n_valid)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdccacf-ea40-434b-b51d-e2fd785f00e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "sample_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "# sample_list = [1]\n",
    "for i in sample_list:\n",
    "    path = f\"model_checkpoint_KO_sample_{i}\"\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"avg_val_loss\",   \n",
    "        dirpath=\"./\", \n",
    "        filename=path,  \n",
    "        save_top_k=1,  \n",
    "        mode=\"min\",    \n",
    "    )\n",
    "    H_train_tensor = torch.tensor(H_train[:, 0:11*i:i, :], dtype=torch.float32)\n",
    "    train_dataset = TensorDataset(H_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    H_valid_tensor = torch.tensor(H_valid[:, 0:11*i:i, :], dtype=torch.float32)\n",
    "    valid_dataset = TensorDataset(H_valid_tensor)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    model = DeepKoopman(input_dim=dim, hidden_dim=hidden_dim, n_layers=n_layers, num_real=num_real, num_complex=num_complex)\n",
    "    lightning_model = TrainModel(model=model, learning_rate=learning_rate, path=path)\n",
    "    trainer = pl.Trainer(accelerator=\"gpu\", devices=4, strategy=\"ddp_notebook\", max_epochs=num_epochs, callbacks=[checkpoint_callback])\n",
    "\n",
    "    trainer.fit(lightning_model, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b72a1b3f-f44f-4ca6-adf9-b9041b669839",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('KO_sample_test.csv', header=None).values\n",
    "length = X_test.shape[1] // n_test\n",
    "H_test = []\n",
    "for i in range(n_test):\n",
    "    H_test.append(X_test[:, i*length:(i+1)*length])\n",
    "H_test = np.stack([H_test[idx].T for idx in range(n_test)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e94052c-a9e7-457a-b50e-60c894f2eb11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "sample_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "error_list = []\n",
    "\n",
    "for i in sample_list:\n",
    "    path = f\"model_checkpoint_KO_sample_{i}.ckpt\"\n",
    "    model = DeepKoopman(input_dim=dim, hidden_dim=hidden_dim, n_layers=n_layers, num_real=num_real, num_complex=num_complex)\n",
    "    lightning_model = TrainModel.load_from_checkpoint(path, model=model, learning_rate=learning_rate, map_location=\"cpu\")\n",
    "    trainer = pl.Trainer(accelerator=\"gpu\", devices=4, strategy=\"ddp_notebook\", max_epochs=num_epochs)\n",
    "    H_test_tensor = torch.tensor(H_test[:, 0:11*i:i, :], dtype=torch.float32)\n",
    "    test_dataset = TensorDataset(H_test_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=9999, shuffle=True)\n",
    "    error_list.append(trainer.test(lightning_model, dataloaders=test_loader)[0]['test_loss'])\n",
    "\n",
    "df = pd.DataFrame(error_list)\n",
    "df.to_csv(\"sample.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13cf3f4a-5760-445c-8b14-ac89ec1dd9da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.34521573781967163,\n",
       " 0.34112581610679626,\n",
       " 0.3411276042461395,\n",
       " 0.35121089220046997,\n",
       " 0.3354300558567047,\n",
       " 0.34621691703796387,\n",
       " 0.34132254123687744,\n",
       " 0.3509877324104309,\n",
       " 0.3417041301727295,\n",
       " 0.34397822618484497,\n",
       " 0.35606205463409424,\n",
       " 0.3498133718967438,\n",
       " 0.3536776006221771,\n",
       " 0.34368571639060974,\n",
       " 0.35125532746315,\n",
       " 0.34510818123817444,\n",
       " 0.3392789363861084,\n",
       " 0.3365575969219208,\n",
       " 0.3411751687526703]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
