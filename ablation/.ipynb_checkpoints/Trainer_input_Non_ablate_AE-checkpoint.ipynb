{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "050e659e-20f4-4b07-b724-21cb2ef26b76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from torch.func import vmap, jacrev\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "import math\n",
    "from pydmd import DMD\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2a57628-6d4f-47de-842f-e5d20fae90a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, n_layers, dropout=0):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
    "        for _ in range(n_layers):\n",
    "            layers += [nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Dropout(dropout)]\n",
    "        layers += [nn.Linear(hidden_dim, output_dim)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, x_dim, u_dim, hidden_dim, n_layers, dropout=0):\n",
    "        super().__init__()\n",
    "        self.encoder = MLP(x_dim + u_dim, x_dim, hidden_dim, n_layers, dropout)\n",
    "        self.decoder = MLP(x_dim + u_dim, x_dim, hidden_dim, n_layers, dropout)\n",
    "\n",
    "    def forward(self, x, u=None, reverse=False):\n",
    "        if u is not None:\n",
    "            x = torch.cat([x, u], dim=-1)\n",
    "        else:\n",
    "            x = x\n",
    "        if not reverse:\n",
    "            return self.encoder(x)\n",
    "        else:\n",
    "            return self.decoder(x)\n",
    "    \n",
    "class CombinedNetwork(nn.Module):\n",
    "    def __init__(self, ae, input_dim, lifted_dim, Xmax, Xmin):\n",
    "        super(CombinedNetwork, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.ae = ae  \n",
    "        self.Xmax = Xmax\n",
    "        self.Xmin = Xmin\n",
    "        self.linear = nn.Linear(input_dim, lifted_dim, bias=False)  \n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def forward(self, x, u=None, reverse=False):\n",
    "        x = x.float()\n",
    "        Xmax = self.Xmax.to(x.device)\n",
    "        Xmin = self.Xmin.to(x.device)\n",
    "        if not reverse:\n",
    "            x = (x - Xmin) / (Xmax - Xmin)\n",
    "            chebyshev = torch.cos(self.linear(torch.arccos(x)))\n",
    "            x = torch.cat((x, chebyshev), dim=-1)\n",
    "            x = self.ae(x, u, reverse)\n",
    "            return x\n",
    "        else:\n",
    "            x = self.ae(x, u, reverse)\n",
    "            x = x[:, :self.input_dim]\n",
    "            x = (Xmax - Xmin) * x + Xmin\n",
    "            return x\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        lambda_s = 5\n",
    "        self.linear.weight.data = torch.distributions.exponential.Exponential(lambda_s).sample(self.linear.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8996c880-1612-417a-ade9-b294f93ad045",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dmd(model, X, U, rank):\n",
    "    GX_pred_list = []\n",
    "    GX_list = []\n",
    "    U_list = []\n",
    "    X_list = []\n",
    "    GX = model(X, U.float())\n",
    "    for i in range(X.shape[0]):\n",
    "        GX_temp = GX[i, :, :].T\n",
    "        dmd = DMD(svd_rank=rank, exact=True, sorted_eigs='abs')\n",
    "        dmd.fit(GX_temp.cpu().detach().numpy())\n",
    "        GX_pred = dmd.reconstructed_data.real\n",
    "        GX_pred = np.array(GX_pred, dtype=np.float32)\n",
    "        GX_pred = torch.from_numpy(GX_pred).cuda()\n",
    "        GX_pred_list.append(GX_pred)\n",
    "        GX_list.append(GX_temp)\n",
    "        U_list.append(U[i, :, :].T)\n",
    "        X_list.append(X[i, :, :].T)\n",
    "    GX_pred = torch.cat(GX_pred_list, dim=-1)\n",
    "    GX = torch.cat(GX_list, dim=1)\n",
    "    U = torch.cat(U_list, dim=-1)\n",
    "    X = torch.cat(X_list, dim=-1)\n",
    "\n",
    "    return GX, GX_pred, U, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bede68be-e48a-4205-b695-7cc047d892cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TrainModel(pl.LightningModule):\n",
    "    def __init__(self, model, rank, learning_rate=1e-3, lamb=1, path=\"model_checkpoint_NP\"):\n",
    "        super(TrainModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.best_val_loss = float('inf') \n",
    "        self.validation_outputs = []\n",
    "        self.lamb = lamb\n",
    "        self.train_losses = []\n",
    "        self.rank = rank\n",
    "        self.path = path+'.ckpt'\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X_batch, U_batch = batch\n",
    "        GY, GY_pred, U, Y = dmd(self.model, X_batch, U_batch, self.rank)\n",
    "        Y_pred = self.model(GY_pred.T, U.T, reverse=True)\n",
    "        loss_lin = self.criterion(GY, GY_pred)\n",
    "        loss_recons = self.criterion(Y.T, Y_pred)\n",
    "\n",
    "        loss = loss_lin + self.lamb * loss_recons\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=False, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        Z_batch, U_batch = batch\n",
    "        Z1, Z_pred, U, Z = dmd(self.model, Z_batch, U_batch, self.rank)\n",
    "        Z_pred = self.model(Z_pred.T, U.T, reverse=True)\n",
    "        valid_loss = self.criterion(Z_pred, Z.T)\n",
    "\n",
    "        self.validation_outputs.append(valid_loss)\n",
    "        self.log('val_loss', valid_loss)\n",
    "        return valid_loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        Z_batch, U_batch = batch\n",
    "        Z1, Z_pred, U, Z = dmd(self.model, Z_batch, U_batch, self.rank)\n",
    "        Z_pred = self.model(Z_pred.T, U.T, reverse=True)\n",
    "        test_loss = self.criterion(Z_pred, Z.T)\n",
    "\n",
    "        self.log('test_loss', test_loss)\n",
    "        return test_loss\n",
    "    \n",
    "    def on_fit_start(self):\n",
    "        if self.trainer.is_global_zero: \n",
    "            if os.path.exists(\"loss_log.txt\"):\n",
    "                os.remove(\"loss_log.txt\")\n",
    "            if os.path.exists(self.path):\n",
    "                os.remove(self.path)\n",
    "    \n",
    "    def on_train_batch_end(self, outputs, batch, batch_idx):\n",
    "        with torch.no_grad():  \n",
    "            for name, module in self.model.named_modules():  \n",
    "                if isinstance(module, nn.Linear): \n",
    "                    if name == \"linear\":  \n",
    "                        continue\n",
    "                    weight = module.weight  \n",
    "                    sigma_max = torch.norm(weight, p=2)  \n",
    "                    if sigma_max > 1:  \n",
    "                        scale = (1 - 1e-3) / sigma_max\n",
    "                        module.weight.data *= scale  \n",
    "    \n",
    "    def on_train_epoch_start(self):\n",
    "        if os.path.exists(self.path):\n",
    "            best_state_dict = torch.load(self.path)[\"state_dict\"]\n",
    "            self.load_state_dict(best_state_dict)\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        if self.trainer.is_global_zero: \n",
    "            avg_train_loss = self.trainer.callback_metrics.get(\"train_loss\")\n",
    "            if avg_train_loss is not None:\n",
    "                self.train_losses.append(avg_train_loss.item())  \n",
    "                print(f\"Epoch {self.current_epoch}: Average Training Loss = {avg_train_loss.item()}\")\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_val_loss = torch.stack(self.validation_outputs).mean()  \n",
    "        self.log('avg_val_loss', avg_val_loss)\n",
    "        self.validation_outputs.clear()\n",
    "        print(f\"Validation loss: {avg_val_loss}\")\n",
    "        with open(\"loss_log.txt\", \"a\") as f:\n",
    "            f.write(f\"{avg_val_loss.item()}\\n\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate, eps=1e-08,\n",
    "                                            weight_decay=0)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=1,\n",
    "            gamma=0.99\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\",  \n",
    "            },\n",
    "            \"gradient_clip_val\": 1.0,  \n",
    "            \"gradient_clip_algorithm\": \"norm\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b602e280-468b-4cf2-a863-55ec84de6de8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dim = 2  \n",
    "hidden_dim = 27  \n",
    "input_dim = 1\n",
    "n_blocks = 3  \n",
    "n_layers = 1\n",
    "n_feature = 8\n",
    "rank = 5\n",
    "batch_size = 512\n",
    "n_train = 10000\n",
    "n_valid = 1000\n",
    "n_test = 1000\n",
    "dropout = 0\n",
    "num_epochs = 100  \n",
    "lamb = 1e-3\n",
    "learning_rate = 1e-3  \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6874831a-16e4-4744-8cc9-1cf2c819a392",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('Non_X_train.csv', header=None).values\n",
    "X_valid = pd.read_csv('Non_X_valid.csv', header=None).values\n",
    "X_test = pd.read_csv('Non_X_test.csv', header=None).values\n",
    "U_train = pd.read_csv('Non_U_train.csv', header=None).values\n",
    "U_valid = pd.read_csv('Non_U_valid.csv', header=None).values\n",
    "U_test = pd.read_csv('Non_U_test.csv', header=None).values\n",
    "\n",
    "length = X_train.shape[1] // n_train\n",
    "HX_train = []\n",
    "HU_train = []\n",
    "for i in range(n_train):\n",
    "    HX_train.append(X_train[:, i*length:(i+1)*length])\n",
    "    HU_train.append(U_train[:, i*length:(i+1)*length])\n",
    "HX_train = np.stack([HX_train[idx].T for idx in range(n_train)], axis=0)\n",
    "HU_train = np.stack([HU_train[idx].T for idx in range(n_train)], axis=0)\n",
    "HX_valid = []\n",
    "HU_valid = []\n",
    "for i in range(n_valid):\n",
    "    HX_valid.append(X_valid[:, i*length:(i+1)*length])\n",
    "    HU_valid.append(U_valid[:, i*length:(i+1)*length])\n",
    "HX_valid = np.stack([HX_valid[idx].T for idx in range(n_valid)], axis=0)\n",
    "HU_valid = np.stack([HU_valid[idx].T for idx in range(n_valid)], axis=0)\n",
    "train_dataset = TensorDataset(torch.tensor(HX_train, dtype=torch.float32), torch.tensor(HU_train, dtype=torch.float32))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "valid_dataset = TensorDataset(torch.tensor(HX_valid, dtype=torch.float32), torch.tensor(HU_valid, dtype=torch.float32))\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=99999, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "X_result = np.concatenate([X_train, X_test, X_valid], axis=-1)\n",
    "Xmax = torch.tensor(np.max(X_result, axis=-1), dtype=torch.float)\n",
    "Xmin = torch.tensor(np.min(X_result, axis=-1), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18639093-8bd2-44b5-82b7-ad2fabbe46d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "path = \"model_checkpoint_NP_ablate_AE\"\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"avg_val_loss\",   \n",
    "    dirpath=\"./ablation\",  \n",
    "    filename=path, \n",
    "    save_top_k=1,  \n",
    "    mode=\"min\",    \n",
    ")\n",
    "ae = AutoEncoder(x_dim=dim+n_feature, u_dim=input_dim, hidden_dim=hidden_dim, n_layers=n_layers, dropout=dropout)\n",
    "model = CombinedNetwork(ae=ae, input_dim=dim, lifted_dim=n_feature, Xmax=Xmax, Xmin=Xmin)\n",
    "lightning_model = TrainModel(model=model, rank=rank, learning_rate=learning_rate, lamb=lamb, path=path)\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", devices=4, strategy=\"ddp_notebook\", max_epochs=num_epochs, callbacks=[checkpoint_callback])\n",
    "\n",
    "trainer.fit(lightning_model, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8abde0b2-c523-4bad-830f-ae0a236d0f44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "ae = AutoEncoder(x_dim=dim+n_feature, u_dim=input_dim, hidden_dim=hidden_dim, n_layers=n_layers, dropout=dropout)\n",
    "model = CombinedNetwork(ae=ae, input_dim=dim, lifted_dim=n_feature, Xmax=Xmax, Xmin=Xmin)\n",
    "path = \"model_checkpoint_NP_ablate_AE.ckpt\"\n",
    "lightning_model = TrainModel.load_from_checkpoint(path, model=model, rank=rank, learning_rate=learning_rate, map_location=\"cpu\")\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", devices=4, strategy=\"ddp_notebook\", max_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54364185-0c2f-433c-9277-17d4419aa82b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "length = X_test.shape[1] // n_test\n",
    "HX_test = []\n",
    "HU_test = []\n",
    "for i in range(n_test):\n",
    "    HX_test.append(X_test[:, i*length:(i+1)*length])\n",
    "    HU_test.append(U_test[:, i*length:(i+1)*length])\n",
    "HX_test = np.stack([HX_test[idx].T for idx in range(n_test)], axis=0)\n",
    "HU_test = np.stack([HU_test[idx].T for idx in range(n_test)], axis=0)\n",
    "test_dataset = TensorDataset(torch.tensor(HX_test, dtype=torch.float32), torch.tensor(HU_test, dtype=torch.float32))\n",
    "test_loader = DataLoader(test_dataset, batch_size=9999, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b65bcc7-4cda-4e60-8fe3-8b08b994f587",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n",
      "You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 4 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.73it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_loss           0.03086288832128048\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.03086288832128048}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "trainer.test(lightning_model, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c46516d-2760-44f1-b2f9-96733ad74163",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "GY, GY_pred, U, _ = dmd(lightning_model.model, torch.tensor(HX_test, dtype=torch.float32), torch.tensor(HU_test, dtype=torch.float32), rank)\n",
    "X_recons = lightning_model.model(GY_pred.cpu().T, U.T, reverse=True).T.detach().numpy()\n",
    "X1 = X_recons.T.reshape(1000, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9593d6d6-e565-4d0c-b1ce-d80371160aef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame((X1 - HX_test.reshape(1000, -1)) ** 2).to_csv('ae.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73e26fce-1add-47b1-b9fa-e80165064c93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03196300690120273"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((X1 - HX_test.reshape(1000, -1)) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9a4fe6-3b49-45b0-80fb-e6f65d6fda97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(X_traj[0, :, 0], X_traj[0, :, 1], label='Test')\n",
    "plt.plot(X_recons[0, :], X_recons[1, :], label='Reconstructed')\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.legend()\n",
    "plt.title('Reconstructed Trajectory on Test Set')\n",
    "# plt.savefig(\"controlled1.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73717888-49d8-4aea-85c6-66e2309b99fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_traj = 1000\n",
    "dim = 2\n",
    "rmse_values = np.zeros(61)\n",
    "for i in range(num_traj):\n",
    "    traj = HX_test[i, :61, :].reshape(1, 61, 2)\n",
    "    GY, GY_pred, U, _ = dmd(lightning_model.model, torch.tensor(traj, dtype=torch.float32), torch.tensor(HU_test[i, :61, :], dtype=torch.float32).reshape(1, 61, 1), rank)\n",
    "    X_recons = lightning_model.model(GY_pred.cpu().T, U.T, reverse=True).detach().numpy()\n",
    "    error = np.sum((X_recons - traj.squeeze()) ** 2, axis=1)  \n",
    "    rmse_values += error\n",
    "rmse_values = np.sqrt(rmse_values / (num_traj * dim)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37895795-f960-4d9a-a206-66acf0703efd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = np.linspace(0, 60*0.1, 61, traj.shape[1])\n",
    "plt.semilogy(t, rmse_values)\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Reconstructed Error on Test Set')\n",
    "plt.savefig(\"controlled2.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa78546d-4a45-4e73-9101-c4472c298a84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_traj = torch.tensor(pd.read_csv('autonomous.csv', header=None).values.T, dtype=torch.float32).unsqueeze(0)\n",
    "U_traj = torch.tensor(np.zeros(X_traj.squeeze(0).shape[:-1] + (1,)), dtype=torch.float32)\n",
    "GX_temp, ldj = model(torch.tensor(X_traj.squeeze(0), dtype=torch.float32), U_traj)\n",
    "GX_temp = GX_temp.T\n",
    "dmd = DMD(svd_rank=rank, exact=True, sorted_eigs='abs')\n",
    "dmd.fit(GX_temp.cpu().detach().numpy())\n",
    "GX_pred = dmd.reconstructed_data.real\n",
    "GX_pred = np.array(GX_pred, dtype=np.float32)\n",
    "GX_pred = torch.from_numpy(GX_pred).cuda()\n",
    "X_recons = lightning_model.model(GX_pred.cpu().T, U_traj, reverse=True).T.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3b8917-e5dd-4164-b1b8-dd572450af4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(X_traj[0, :, 0], X_traj[0, :, 1], label='Test')\n",
    "plt.plot(X_recons[0, :], X_recons[1, :], label='Reconstructed')\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.legend()\n",
    "plt.title('Reconstructed Trajectory Without Input')\n",
    "plt.savefig(\"controlled3.png\", dpi=300, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
